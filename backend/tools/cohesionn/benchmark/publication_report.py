"""
Publication Report Generator
============================
Generates a structured markdown document from full benchmark results.

Output is suitable for use as marketing material and technical documentation.
References chart PNGs generated by visualizations.py.
"""

import time
from pathlib import Path
from typing import Any, Dict, List, Optional


def generate_publication_report(
    report_data: Dict[str, Any],
    output_dir: str,
    hardware_info: Optional[Dict[str, str]] = None,
) -> str:
    """
    Generate a publication-quality markdown report.

    Args:
        report_data: Full results.json data from ShootoutReport
        output_dir: Directory containing charts and results
        hardware_info: Optional hardware details override

    Returns:
        Path to the generated report.md file
    """
    output = Path(output_dir)
    hw = hardware_info or {
        "gpu": "NVIDIA RTX 5090 (32GB GDDR7)",
        "cpu": "16-core",
        "ram": "64GB DDR5",
    }

    sections = [
        _header(),
        _methodology(report_data, hw),
        _ingestion_section(report_data),
        _reranker_section(report_data),
        _embedding_section(report_data),
        _cross_matrix_section(report_data),
        _param_sweep_section(report_data),
        _llm_section(report_data),
        _ablation_section(report_data),
        _profile_section(report_data),
        _optimal_config(report_data),
        _tier_analysis(report_data),
        _appendix(report_data),
    ]

    md = "\n\n".join(s for s in sections if s)
    report_path = output / "report.md"
    report_path.write_text(md, encoding="utf-8")
    return str(report_path)


def _header() -> str:
    return f"""# ARCA RAG Pipeline Benchmark Report

*Generated {time.strftime("%Y-%m-%d %H:%M:%S")}*

> Comprehensive evaluation of retrieval-augmented generation pipeline components
> for the ARCA platform, testing embedding models, rerankers, LLMs, ingestion
> settings, and RAG pipeline feature toggles."""


def _methodology(data: Dict, hw: Dict) -> str:
    config = data.get("config", {})
    phases = data.get("phases", {})
    total_variants = sum(p.get("n_variants", 0) for p in phases.values())

    return f"""## 1. Methodology

### Hardware
| Component | Specification |
|-----------|--------------|
| GPU | {hw.get("gpu", "N/A")} |
| CPU | {hw.get("cpu", "N/A")} |
| RAM | {hw.get("ram", "N/A")} |

### Corpus
- **Document**: {config.get("cfem_path", "User-provided corpus") or "User-provided corpus"}
- **Topic**: `{config.get("topic", "general")}`
- **Queries**: {len(config.get("queries", [])) or "10+ (generic + domain-specific)"} across difficulty tiers (factual, conceptual, multi_hop, negation, plus domain tiers)

### Scoring
| Metric | Weight | Description |
|--------|--------|-------------|
| Keyword Hit Rate | 35% | Fraction of expected keywords found in retrieved text |
| Entity Hit Rate | 15% | Fraction of expected named entities found |
| MRR | 20% | Mean Reciprocal Rank of first relevant chunk |
| nDCG | 15% | Normalized Discounted Cumulative Gain |
| Source Diversity | 10% | Unique sources / total chunks |
| Latency | 5% | Normalized retrieval speed (0ms=1.0, 2000ms=0.0) |

**Composite** = 0.35×keyword + 0.15×entity + 0.20×MRR + 0.15×nDCG + 0.10×diversity + 0.05×latency

### Phases Tested
{_bullet_list([f"**{name}**: {p.get('n_variants', 0)} variants, {p.get('duration_s', 0):.0f}s" for name, p in phases.items()])}

**Total variants tested**: {total_variants}"""


def _ingestion_section(data: Dict) -> Optional[str]:
    phases = data.get("phases", {})
    if "ingestion" not in phases:
        return None

    phase = phases["ingestion"]
    variants = phase.get("variants", [])
    ranking = phase.get("ranking", [])

    if not ranking:
        return None

    rows = []
    for v in variants:
        if v.get("error"):
            continue
        meta = v.get("metadata", {})
        agg = v.get("aggregate", {})
        rows.append(
            f"| {v['variant_name']} | {meta.get('extractor', 'N/A')} | "
            f"{meta.get('chunk_size', 'N/A')} | {meta.get('chunk_overlap', 'N/A')} | "
            f"{meta.get('chunks_created', 'N/A')} | "
            f"{agg.get('avg_composite', 0):.3f} | {agg.get('avg_keyword_hits', 0):.1%} | "
            f"{meta.get('ingest_time_s', 0):.0f}s |"
        )

    winner = ranking[0] if ranking else {}

    return f"""## 2. Ingestion Optimization

![Ingestion Matrix](chart_ingestion_matrix.png)

### Results

| Variant | Extractor | Chunk Size | Overlap | Chunks | Composite | Keywords | Ingest Time |
|---------|-----------|------------|---------|--------|-----------|----------|-------------|
{chr(10).join(rows)}

**Winner**: `{winner.get("variant", "N/A")}` — composite {winner.get("composite", 0):.3f}

![Chunk Distribution](chart_chunk_distribution.png)"""


def _reranker_section(data: Dict) -> Optional[str]:
    return _model_comparison_section(
        data, "reranker", "3. Reranker Comparison", "chart_rerankers.png"
    )


def _embedding_section(data: Dict) -> Optional[str]:
    return _model_comparison_section(
        data, "embedding", "4. Embedding Model Comparison", "chart_embedders.png"
    )


def _model_comparison_section(
    data: Dict, phase_name: str, title: str, chart_name: str
) -> Optional[str]:
    phases = data.get("phases", {})
    if phase_name not in phases:
        return None

    phase = phases[phase_name]
    ranking = phase.get("ranking", [])
    variants = {v["variant_name"]: v for v in phase.get("variants", []) if v.get("aggregate")}

    if not ranking:
        return None

    rows = []
    for r in ranking:
        v = variants.get(r["variant"], {})
        agg = v.get("aggregate", {})
        spec = v.get("model_spec", {})
        rows.append(
            f"| {r['rank']} | {r['variant']} | "
            f"{agg.get('avg_composite', 0):.3f} | {agg.get('avg_keyword_hits', 0):.1%} | "
            f"{agg.get('avg_mrr', 0):.3f} | {agg.get('avg_ndcg', 0):.3f} | "
            f"{agg.get('avg_latency_ms', 0):.0f}ms | "
            f"{spec.get('notes', '')} |"
        )

    winner = ranking[0]

    return f"""## {title}

![{title}]({chart_name})

| Rank | Model | Composite | Keywords | MRR | nDCG | Latency | Notes |
|------|-------|-----------|----------|-----|------|---------|-------|
{chr(10).join(rows)}

**Winner**: `{winner["variant"]}` — composite {winner["composite"]:.3f}"""


def _cross_matrix_section(data: Dict) -> Optional[str]:
    phases = data.get("phases", {})
    if "cross_matrix" not in phases:
        return None

    phase = phases["cross_matrix"]
    ranking = phase.get("ranking", [])

    if not ranking:
        return None

    rows = []
    for r in ranking[:10]:
        rows.append(f"| {r['rank']} | {r['variant']} | {r['composite']:.3f} |")

    return f"""## 5. Cross-Matrix: Embedding x Reranker

![Cross-Matrix Heatmap](chart_cross_matrix.png)

### Top Combinations

| Rank | Combination | Composite |
|------|-------------|-----------|
{chr(10).join(rows)}

**Best pair**: `{ranking[0]["variant"]}` — composite {ranking[0]["composite"]:.3f}"""


def _param_sweep_section(data: Dict) -> Optional[str]:
    phases = data.get("phases", {})
    if "param_sweep" not in phases:
        return None

    phase = phases["param_sweep"]
    variants = phase.get("variants", [])

    if not variants:
        return None

    # Group by parameter
    params: Dict[str, List] = {}
    for v in variants:
        if v.get("error") or not v.get("metadata"):
            continue
        param = v["metadata"].get("param", "unknown")
        params.setdefault(param, []).append(v)

    rows = []
    for param_name, pvariants in sorted(params.items()):
        pvariants.sort(key=lambda v: v.get("aggregate", {}).get("avg_composite", 0), reverse=True)
        best = pvariants[0] if pvariants else None
        if best:
            best_val = best["metadata"].get("value", "N/A")
            best_score = best.get("aggregate", {}).get("avg_composite", 0)
            rows.append(f"| {param_name} | {best_val} | {best_score:.3f} |")

    return f"""## 6. Parameter Sensitivity

![Parameter Sweep](chart_param_sweep.png)

### Optimal Values

| Parameter | Best Value | Composite |
|-----------|-----------|-----------|
{chr(10).join(rows)}"""


def _llm_section(data: Dict) -> Optional[str]:
    phases = data.get("phases", {})
    if "llm" not in phases:
        return None

    phase = phases["llm"]
    ranking = phase.get("ranking", [])
    variants = {v["variant_name"]: v for v in phase.get("variants", []) if v.get("aggregate")}

    if not ranking:
        return None

    rows = []
    for r in ranking:
        v = variants.get(r["variant"], {})
        agg = v.get("aggregate", {})
        rows.append(
            f"| {r['rank']} | {r['variant']} | "
            f"{agg.get('avg_composite', 0):.3f} | {agg.get('avg_keyword_hits', 0):.1%} | "
            f"{agg.get('avg_latency_ms', 0):.0f}ms |"
        )

    return f"""## 7. LLM Generation Quality

![LLM Comparison](chart_llm.png)

| Rank | Model | Composite | Keywords | Latency |
|------|-------|-----------|----------|---------|
{chr(10).join(rows)}

**Winner**: `{ranking[0]["variant"]}` — composite {ranking[0]["composite"]:.3f}"""


def _ablation_section(data: Dict) -> Optional[str]:
    phases = data.get("phases", {})
    if "ablation" not in phases:
        return None

    phase = phases["ablation"]
    variants = phase.get("variants", [])

    if not variants:
        return None

    rows = []
    for v in sorted(variants, key=lambda x: abs(x.get("metadata", {}).get("delta", 0)), reverse=True):
        if v.get("error"):
            continue
        meta = v.get("metadata", {})
        delta = meta.get("delta", 0)
        direction = "+" if delta > 0 else ""
        rows.append(
            f"| {meta.get('label', v['variant_name'])} | "
            f"{meta.get('on_composite', 0):.3f} | {meta.get('off_composite', 0):.3f} | "
            f"{direction}{delta:.3f} | {'Helps' if delta > 0.005 else 'Hurts' if delta < -0.005 else 'Neutral'} |"
        )

    return f"""## 8. Ablation Studies

![Ablation Results](chart_ablation.png)

| Component | ON Score | OFF Score | Delta | Impact |
|-----------|----------|-----------|-------|--------|
{chr(10).join(rows)}

Components are sorted by absolute impact. A positive delta means the component improves retrieval quality."""


def _profile_section(data: Dict) -> Optional[str]:
    """Generate profile comparison section for the report."""
    phases = data.get("phases", {})
    if "profile_comparison" not in phases:
        return None

    phase = phases["profile_comparison"]
    variants = phase.get("variants", [])

    if not variants:
        return None

    rows = []
    for v in sorted(variants, key=lambda x: x.get("aggregate", {}).get("avg_composite", 0), reverse=True):
        if v.get("error"):
            continue
        agg = v.get("aggregate", {})
        meta = v.get("metadata", {})
        rows.append(
            f"| {v['variant_name']} | {agg.get('avg_keyword_hits', 0):.1%} "
            f"| {agg.get('avg_composite', 0):.3f} "
            f"| {agg.get('avg_latency_ms', 0):.0f}ms |"
        )

    if not rows:
        return None

    return f"""## 8b. Profile Comparison

| Profile | Keyword Accuracy | Composite | Avg Latency |
|---------|-----------------|-----------|-------------|
{chr(10).join(rows)}

Profiles control which pipeline stages are active. "Fast" enables dense retrieval + reranking + query expansion + domain boost. "Deep" enables the full 14-stage pipeline including HyDE, BM25, RAPTOR, GraphRAG, and global community search."""


def _optimal_config(data: Dict) -> str:
    phases = data.get("phases", {})
    overall = data.get("overall_winner") or {}

    rows = []
    for phase_name in ["ingestion", "reranker", "embedding", "cross_matrix", "param_sweep", "llm"]:
        if phase_name in phases:
            phase = phases[phase_name]
            winner = phase.get("winner") or "N/A"
            ranking = phase.get("ranking") or []
            score = ranking[0]["composite"] if ranking else 0
            rows.append(f"| {phase_name} | {winner} | {score:.3f} |")

    return f"""## 9. Optimal Configuration

| Phase | Winner | Composite |
|-------|--------|-----------|
{chr(10).join(rows)}

**Overall champion**: `{overall.get("variant", "N/A")}` ({overall.get("phase", "N/A")}) — composite {overall.get("composite", 0):.3f}"""


def _tier_analysis(data: Dict) -> Optional[str]:
    phases = data.get("phases") or {}
    overall_ranking = data.get("overall_ranking") or []

    if len(overall_ranking) < 2:
        return None

    # Get top 3 unique configs
    top_configs = []
    seen = set()
    for r in overall_ranking:
        key = f"{r['phase']}:{r['variant']}"
        if key not in seen:
            seen.add(key)
            # Find variant data
            phase_data = phases.get(r["phase"], {})
            for v in phase_data.get("variants", []):
                if v["variant_name"] == r["variant"] and v.get("aggregate"):
                    top_configs.append((r["variant"], v["aggregate"].get("by_tier", {})))
                    break
        if len(top_configs) >= 3:
            break

    if not top_configs:
        return None

    # Build tier comparison table
    all_tiers = sorted(set(
        tier for _, by_tier in top_configs for tier in by_tier
    ))

    header = "| Tier | " + " | ".join(name for name, _ in top_configs) + " |"
    sep = "|------|" + "|".join("------" for _ in top_configs) + "|"
    rows = []
    for tier in all_tiers:
        vals = []
        for _, by_tier in top_configs:
            score = by_tier.get(tier, {}).get("avg_composite", 0)
            vals.append(f"{score:.3f}")
        rows.append(f"| {tier} | " + " | ".join(vals) + " |")

    return f"""## 10. Tier Analysis

![Tier Breakdown](chart_tier_breakdown.png)

{header}
{sep}
{chr(10).join(rows)}"""


def _appendix(_data: Dict) -> str:
    return """## Appendix

### Reproducibility
All benchmark code is in `backend/tools/cohesionn/benchmark/`. Run with:
```bash
docker exec -t arca-backend python /app/scripts/benchmark_full_pipeline.py --phases all
```

### Data Files
- `results.json` — Full structured results for all phases
- `report.md` — This document
- `chart_*.png` — All visualizations

### Methodology Notes
- Each tier contributes equally to the composite score (tier-weighted aggregation)
- Latency normalization: 0ms → 1.0, ≥2000ms → 0.0
- For ingestion tests, the topic is fully cleared and re-ingested between variants
- Ablation tests restore original config after each toggle
- All tests use the same 30-query battery for consistency"""


def _bullet_list(items: List[str]) -> str:
    return "\n".join(f"- {item}" for item in items)
