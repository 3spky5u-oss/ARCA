# ARCA Configuration
# Copy to .env and customize

# ===================
# Domain Pack
# ===================
# Which domain pack to load. "example" = vanilla ARCA, no domain tools.
# Domain packs live in ./domains/{name}/ with a manifest.json.
ARCA_DOMAIN=example

# ===================
# Container Images
# ===================
# Public image flow: docker compose pull && docker compose up -d
# For local development changes, use: docker compose up -d --build
ARCA_IMAGE_TAG=latest
ARCA_BACKEND_IMAGE=ghcr.io/3spky5u-oss/arca-backend
ARCA_FRONTEND_IMAGE=ghcr.io/3spky5u-oss/arca-frontend
# ARCA_PULL_POLICY=missing

# ===================
# LLM Models (llama.cpp via llama-server)
# ===================
# Chat model (GGUF file in ./models/)
# First-run bootstrap auto-selects chat/code/expert + vision defaults by detected total GPU VRAM.
# Ranges: 0-8, 8-12, 12-16, 16-24, 24-32, 32-48, 48-96, 96+ GB.
# You can always override manually.
LLM_CHAT_MODEL=Qwen3-30B-A3B-Q4_K_M.gguf
LLM_CHAT_MODEL_REPO=unsloth/Qwen3-30B-A3B-GGUF

# Code model (defaults to chat model; separate if you want a dedicated coder GGUF)
LLM_CODE_MODEL=Qwen3-30B-A3B-Q4_K_M.gguf
LLM_CODE_MODEL_REPO=unsloth/Qwen3-30B-A3B-GGUF

# Bootstrap download defaults (used by scripts/model_bootstrap.py)
ARCA_DEFAULT_CHAT_REPO=unsloth/Qwen3-30B-A3B-GGUF
ARCA_DEFAULT_CHAT_FILE=Qwen3-30B-A3B-Q4_K_M.gguf

# Expert/think model (higher quality, used for calculations)
LLM_EXPERT_MODEL=Qwen3-30B-A3B-Q4_K_M.gguf
LLM_EXPERT_MODEL_REPO=unsloth/Qwen3-30B-A3B-GGUF

# Vision model (multimodal, used for document OCR)
LLM_VISION_MODEL=Qwen3VL-8B-Instruct-Q8_0.gguf
LLM_VISION_MODEL_REPO=Qwen/Qwen3-VL-8B-Instruct-GGUF
LLM_VISION_STRUCTURED_MODEL=Qwen3VL-8B-Instruct-Q8_0.gguf
LLM_VISION_STRUCTURED_MODEL_REPO=Qwen/Qwen3-VL-8B-Instruct-GGUF

# Auto-download missing GGUF files at backend startup.
# Defaults now pull all configured slots (chat/code/expert/vision/vision_structured)
# plus required vision mmproj when missing.
ARCA_AUTO_DOWNLOAD_MODELS=true
ARCA_AUTO_DOWNLOAD_OPTIONAL_MODELS=true

# Optional startup check: probe llama-server /health endpoints during validation.
# Usually left false because startup validation runs before llama-server startup.
ARCA_VALIDATE_LLM_HEALTH=false

# Optional Hugging Face token (needed for gated/private repos)
# HF_TOKEN=
# HUGGING_FACE_HUB_TOKEN=

# Fine-tuned model for A/B testing (empty = disabled, uses base model)
# LLM_FINETUNED_MODEL=

# ===================
# Services
# ===================
SEARXNG_URL=http://searxng:8080
SEARXNG_ENABLED=true
SEARXNG_CATEGORIES=general
# Optional (blank = SearXNG auto language)
SEARXNG_LANGUAGE=
SEARXNG_TIMEOUT_S=10
SEARXNG_MAX_RESULTS=5
# json preferred; html is fallback-friendly for restricted instances
SEARXNG_REQUEST_FORMAT=json

# ===================
# Cleanup & Maintenance
# ===================
# Wipe sessions, uploads, and reports on each restart (default: false)
# Enable for production multi-user deployments. Also toggleable in Admin > Config > General.
CLEANUP_ON_STARTUP=false
CLEANUP_MAX_AGE_HOURS=24
CLEANUP_INTERVAL_HOURS=6

# ===================
# Performance Tuning
# ===================

# Dynamic context window (auto-adjusts based on task complexity)
LLM_CTX_SMALL=4096      # Simple tools (unit_convert, lookup_guideline)
LLM_CTX_MEDIUM=8192     # Standard chat
LLM_CTX_LARGE=16384     # RAG-heavy tasks
LLM_CTX_XLARGE=24576    # Think mode, code generation

# LLM inference parameters
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_TOP_K=40

# ===================
# RAG Embedding & Reranker
# ===================

# Benchmark-validated defaults (shootout_20260208_211323)
# Cross-matrix champion pairing: qwen3-06b + jina-v2-turbo (0.752 composite)
COHESIONN_EMBED_MODEL=Qwen/Qwen3-Embedding-0.6B
COHESIONN_RERANK_MODEL=jinaai/jina-reranker-v2-base-multilingual
COHESIONN_RERANK_MAX_LENGTH=1024

# Backend: "onnx" (default, low memory) or "torch" (legacy, +17GB host RAM overhead)
# ONNX eliminates PyTorch CUDA context allocation. Falls back to torch automatically
# if ONNX export fails for a specific model.
# COHESIONN_EMBED_BACKEND=onnx
# COHESIONN_RERANK_BACKEND=onnx

# Device override (auto-detects CUDA by default)
COHESIONN_EMBED_DEVICE=cuda
COHESIONN_RERANK_DEVICE=cuda

# ===================
# Admin Panel
# ===================

# Admin password is set via the admin UI on first run.
# Access admin panel: Ctrl+Shift+D from main app, or navigate to /admin

# Set to true and restart to reset the admin password (forces setup screen)
# ADMIN_RESET=false

# ===================
# Hardware Tuning
# ===================

# Multi-GPU device map — route components to specific GPUs
# Format: component:gpu_id pairs, comma-separated
# Components: chat, vision, expert, embedder, reranker
# Single GPU: leave unset (everything routes to GPU 0 automatically)
# ARCA_DEVICE_MAP=chat:0,vision:1,embedder:2,reranker:2

# Optional: tensor split a single llama.cpp slot across multiple GPUs.
# split_mode: none|layer|row
# tensor_split: comma-separated per-GPU ratios (examples: 1,1 or 3,1)
# If set globally, applies to chat + vision unless slot-specific values override.
# LLM_SPLIT_MODE=layer
# LLM_TENSOR_SPLIT=1,1
# Slot-specific overrides:
# LLM_CHAT_SPLIT_MODE=layer
# LLM_CHAT_TENSOR_SPLIT=1,1
# LLM_VISION_SPLIT_MODE=none
# LLM_VISION_TENSOR_SPLIT=

# GPU layer offloading (-1 = all layers to GPU, 0 = CPU only)
# LLM_CHAT_GPU_LAYERS=-1
# LLM_VISION_GPU_LAYERS=-1

# Optional MoE CPU expert offload for llama.cpp (useful for large MoE models).
# Set globally or per-slot (chat/vision). Example for future OSS-120B experiments:
# LLM_N_CPU_MOE=16
# LLM_CHAT_N_CPU_MOE=16
# LLM_VISION_N_CPU_MOE=

# Lock model weights in RAM (prevents OS paging, requires sufficient RAM)
# LLM_MLOCK=false

# Startup timeout in seconds (increase for slower hardware)
# LLM_STARTUP_TIMEOUT=300

# Pre-cache on-demand models into OS page cache at startup
# LLM_PRECACHE_MODELS=true

# Keep vision model server always running (uses ~6GB extra VRAM)
# LLM_VISION_ALWAYS_RUNNING=false

# KV cache quantization (q8_0 = half memory vs f16, minimal quality loss)
# LLM_CACHE_TYPE_K=q8_0
# LLM_CACHE_TYPE_V=q8_0

# Memory limits — prevents runaway processes from freezing your OS
# BACKEND_MEMORY_LIMIT=32G      # Hard ceiling for backend container (CUDA + Python + models)
# REDIS_MAXMEMORY=1gb
# QDRANT_MEMORY_LIMIT=8G

# CUDA architectures for llama-server build (docker compose build --build-arg CUDA_ARCHS="...")
# 75=Turing(RTX20xx) 80=Ampere(A100) 86=Ampere(RTX30xx) 89=Ada(RTX40xx) 120=Blackwell(RTX50xx)
# Default builds for 75;80;86;89. Add your arch if not listed.

# ===================
# MCP (Model Context Protocol)
# ===================
# MCP-only runtime mode.
# true  = disable local chat responses and skip local llama-server startup/validation.
# false = normal ARCA chat + tools mode.
MCP_MODE=false

# Static API key for the MCP server adapter (Claude Desktop integration).
# Set this to a random string, then use the same value in Claude Desktop's
# claude_desktop_config.json as ARCA_MCP_KEY. Leave empty to disable MCP endpoints.
# MCP_API_KEY=

# Built-in ARCA self-knowledge corpus (ingested from data/core_knowledge/*.md)
# Keep enabled for best "what can ARCA do?" responses.
CORE_KNOWLEDGE_ENABLED=true
CORE_KNOWLEDGE_COLLECTION=arca_core

# ===================
# Database Passwords
# ===================
NEO4J_PASSWORD=change-me-in-production
POSTGRES_PASSWORD=change-me-in-production

# PostgreSQL startup race handling (backend waits briefly before SQLite fallback)
POSTGRES_CONNECT_RETRIES=15
POSTGRES_CONNECT_RETRY_DELAY_S=2.0
